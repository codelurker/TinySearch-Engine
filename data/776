http://www.cs.dartmouth.edu/~ac/DTS/spring04.html
3
<html><head><title>DTS: Dartmouth Theory Seminars - Spring 2004</title></head>
<body background="../dartback.png" link=#0000ff vlink=#0000ff>

<h2>DTS: Dartmouth Theory Seminars - Spring 2004</h2>

<h3>Spring 2004 Schedule</h3>

<table cellpadding=3><tr>
  <td valign=top>Apr 19</td>
  <td>Adam Klivans, Harvard University<br>
  <a href="#klivans">Learning Halfspaces: New Algorithms and 
  Applications</a><br>
  Room: Carson L01<br>
  Time: 4pm - 5pm.
</tr><tr>
  <td valign=top>Apr 26</td>
  <td>Graham Cormode, DIMACS, Rutgers University<br>
  <a href="#cormode">Algorithms for Processing Massive Data at Network
  Line Speed</a><br>
  Room: Carson L01<br>
  Time: 4pm - 5pm.
</tr><tr>
  <td valign=top>May 10</td>
  <td>Santosh Vempala, MIT<br>
  <a href="#vempala">A Simple Polynomial-Time Rescaling Algorithm for
  Solving Linear Programs</a><br>
  Room: Moore B03<br>
  Time: 4pm - 5pm.
</tr></table>

<p><hr><p>

<h3>Abstracts</h3>

<a name="klivans">
  <b>Adam Klivans, Harvard University<br>
  Learning Halfspaces: New Algorithms and Applications</b><p>

  Learning a halfspace is one of the oldest and most fundamental tasks
  in machine learning.  It is the essential underlying algorithm for
  many systems used by practitioners from fields such as artificial
  intelligence, statistics, and computational biology to classify data.
  In this talk we will give several new approaches for learning
  halfspaces and describe surprising algorithmic applications.<p>

  First, we will show that learning a halfspace in high dimensions
  yields fast algorithms for learning Boolean concept classes such as
  DNF formulae, one of the major challenges in computational learning
  theory.  To this end we will show how to obtain the fastest known DNF
  learning algorithm.<p>

  Secondly, we will describe two new algorithms for learning
  intersections of halfspaces. Intersections of halfspaces are a
  powerful concept class; any convex body can be expressed as an
  intersection of halfspaces, and little is known about the complexity
  of learning the intersection of even two halfspaces.  We give the
  first polynomial-time algorithms for learning the intersection of any
  constant number of halfspaces assuming either a margin separating
  positive points from negative points or that the points are chosen
  uniformly at random.<p>

<hr width=50%><p>

<a name="cormode">
  <b>Graham Cormode, DIMACS, Rutgers University<br>
  Algorithms for Processing Massive Data at Network Line Speed</b><p>

  Many fundamental data mining problems gain additional complexity when
  applied to massive amounts data that is being generated at high speed.
  Networks are a driving force in massive data analysis, and require new
  approaches to produce the analyses network managers require.  This
  motivates the data stream approach: using a small amount of space to
  process data very quickly in one pass. This has resulted in many
  strong and deep algorithmic results in this model of computation.  But
  there is frequently a gap between theory and practice, since existing
  algorithms are too slow for typical high data rates by several orders
  of magnitude.<p>

  I will describe my recent work aimed at bridging the gap.  Firstly, I
  will describe the Count-Min sketch, which is a simple and hence fast
  data structure that can be used to approximate entries of a high
  dimensional vector in very small space.  It can be used as a "black
  box" to solve several problems, including finding frequent items, and
  quantiles of the data distribution, and gives provable guarantees
  about their results.  Secondly, I will describe and analyze a solution
  to the change detection problem, where the aim is to identify items
  (e.g., IP addresses) whose behavior has changed between two periods.
  Both these methods have been implemented and are running on network
  data at network line speeds with high accuracy.<p>

  Joint work with S. Muthukrishnan.<p>

<hr width=50%><p>

<a name="vempala">
  <b>Santosh Vempala, MIT<br>
  A Simple Polynomial-Time Rescaling Algorithm for Solving Linear
  Programs</b><p>

  The perceptron algorithm, developed mainly in the machine learning
  literature, is a simple greedy method for finding a feasible solution   
  to a linear program (alternatively, for learning a linear threshold
  function). In spite of its exponential worst-case complexity, it is  
  often quite useful, in part due to its noise-tolerance and its
  overall simplicity. In this talk, we show that a randomized version
  of the perceptron algorithm with periodic rescaling runs in
  polynomial-time.  The resulting method for solving linear programms
  has an elementary description and analysis. The talk will not assume
  prior knowledge of linear programs.<p>

  Joint work with John Dunagan (Microsoft Research).<p>

<hr><p>

<p align=right> <i> Back to <a href="index.html">DTS main page</a>. </i>
</p>

</body></html>
i