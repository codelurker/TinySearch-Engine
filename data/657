http://www.cs.dartmouth.edu/reports/abstracts/TR94-224
3
<HTML>
<!-- WARNING! Do not modify this file; it is
      automatically generated from a database in www/TRdata/. -->
<link rel="alternate" type="application/rss+xml" title="Dartmouth Computer Science Technical Reports" href="http://www.cs.dartmouth.edu/reports/feed.xml"/>
<HEAD>
<TITLE> Dartmouth PCS-TR94-224 </TITLE>
<META NAME="description" CONTENT="BMMC Permutations on a DECmpp 12000/sx 2000">
<META NAME="number" CONTENT="PCS-TR94-224">
<META NAME="year" CONTENT="1994">
<META NAME="title" CONTENT="BMMC Permutations on a DECmpp 12000/sx 2000">
<META NAME="abstract" CONTENT="  Increasingly, modern computing problems, including many scientific
and business applications, require huge amounts of data to be
examined, modified, and stored.  Parallel computers can be used to
decrease the time needed to operate on such large data sets, by
allowing computations to be performed on many pieces of data at once.
For example, on the DECmpp machine used in our research, there are
2048 processors in the parallel processor array.  The DECmpp can read
data into each of these processors, perform a computation in parallel
on all of it, and write the data out again, theoretically decreasing
the execution time by a factor of 2048 over the time required by one
of its processors.
  Often, the computations that occur after the data is in the
processors involve rearranging, or permuting, the data within the
array of parallel processors.  Information moves between processors by
means of a network connecting them.  Communication through the network
can be very expensive, especially if there are many
collisions--simultaneous contentions for the same network
resource--between items of data moving from one processor to another.
When a program performs hundreds or even thousands of these
permutations during its execution, a bottleneck can occur, impeding
the overall performance of the program.
  Effective algorithms that decrease the time required to permute the
data within a parallel computer can yield a significant speed increase
in running programs with large data sets.  Cormen has designed
algorithms to improve performance when the data movement is defined by
certain classes of permutations.  This thesis will examine the
performance of one of these classes, the
bit-matrix-multiply/complement (BMMC) permutation, when implemented on
the DECmpp.  Although Cormen's algorithm was designed for parallel
disk systems, this thesis adapts it to permutations of data residing
in the memory of the parallel processors.
  The DECmpp network follows the model of an Extended Delta Network
(EDN).  One characteristic of an EDN is that it has a set of input and
output ports to the network, each of which can carry only one item of
data at a time.  If more than one item needs to travel over a given
port, a collision occurs.  The data must access the port serially,
which slows down the entire operation.  Cormen's algorithm reduces
these collisions by computing a schedule for sending the data over the
network.
  For small data sets, it is not worthwhile to perform the extra
operations to generate such a schedule, because the overhead
associated with computing the schedule outweighs the time gained by
preventing collisions at the network ports.  As the size of the data
set increases, eliminating collisions becomes more and more valuable.
On the DECmpp, when the data permutation involves more than 128
elements per processor, our algorithm beats the more naive and obvious
method for permuting in the parallel processor array.">
<META NAME="author" CONTENT="Kristin Bruhl">
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<!-- Include file for abstracts/*.shtml, abstracts/*/*.shtml and authors/*.shtml -->
<!-- See also footer.htmlf -->
<!-- I use absolute pathnames here because this file is used from several directories. -->
<table border=0 rules=none>
<col width=85><col width=100%><col width=85>
<tr valign=center  bgcolor=#003300 border=0>
 <td>
   <a href=/><img src=/reports/Dartmouth2.png alt="Dartmouth logo"
   width=82 height=91 border=0></a>
 </td>
 <td>
   <font color=#ffffff size=+3>
     Dartmouth College Computer Science<br>
     Technical Report series
   </font>
 </td>
 <td>
  <a href="/"><font color=#ffffff>CS&nbsp;home</font></a><br>
  <a href="/reports"><font color=#ffffff>TR&nbsp;home</font></a><br>
  <a href="/reports/#search"><font color=#ffffff>TR&nbsp;search</font></a>
  <a href="/reports/updates.html"><font color=#ffffff>TR&nbsp;listserv</font></a>
 </td>
</tr>
<tr>
  <td align=right> By author:
  </td>
  <td colspan=2>
    <a href="/reports/authors/index.shtml#A">A</a>
    <a href="/reports/authors/index.shtml#B">B</a>
    <a href="/reports/authors/index.shtml#C">C</a>
    <a href="/reports/authors/index.shtml#D">D</a>
    <a href="/reports/authors/index.shtml#E">E</a>
    <a href="/reports/authors/index.shtml#F">F</a>
    <a href="/reports/authors/index.shtml#G">G</a>
    <a href="/reports/authors/index.shtml#H">H</a>
    <a href="/reports/authors/index.shtml#I">I</a>
    <a href="/reports/authors/index.shtml#J">J</a>
    <a href="/reports/authors/index.shtml#K">K</a>
    <a href="/reports/authors/index.shtml#L">L</a>
    <a href="/reports/authors/index.shtml#M">M</a>
    <a href="/reports/authors/index.shtml#N">N</a>
    <a href="/reports/authors/index.shtml#O">O</a>
    <a href="/reports/authors/index.shtml#P">P</a>
    <a href="/reports/authors/index.shtml#Q">Q</a>
    <a href="/reports/authors/index.shtml#R">R</a>
    <a href="/reports/authors/index.shtml#S">S</a>
    <a href="/reports/authors/index.shtml#T">T</a>
    <a href="/reports/authors/index.shtml#U">U</a>
    <a href="/reports/authors/index.shtml#V">V</a>
    <a href="/reports/authors/index.shtml#W">W</a>
    <a href="/reports/authors/index.shtml#X">X</a>
    <a href="/reports/authors/index.shtml#Y">Y</a>
    <a href="/reports/authors/index.shtml#Z">Z</a>
   </td>
</tr>
<tr>
  <td align=right> By number: 
  </td>
  <td colspan=2>
    <font size=-1>
      <a href="/reports/abstracts/2009.shtml">2009</a>,
      <a href="/reports/abstracts/2008.shtml">2008</a>,
      <a href="/reports/abstracts/2007.shtml">2007</a>,
      <a href="/reports/abstracts/2006.shtml">2006</a>,
      <a href="/reports/abstracts/2005.shtml">2005</a>,
      <a href="/reports/abstracts/2004.shtml">2004</a>,
      <a href="/reports/abstracts/2003.shtml">2003</a>,
      <a href="/reports/abstracts/2002.shtml">2002</a>,
      <a href="/reports/abstracts/2001.shtml">2001</a>,
      <a href="/reports/abstracts/2000.shtml">2000</a>,
      <a href="/reports/abstracts/1999.shtml">1999</a>,
      <a href="/reports/abstracts/1998.shtml">1998</a>,
      <a href="/reports/abstracts/1997.shtml">1997</a>,
      <a href="/reports/abstracts/1996.shtml">1996</a>,
      <a href="/reports/abstracts/1995.shtml">1995</a>,
      <a href="/reports/abstracts/1994.shtml">1994</a>,
      <a href="/reports/abstracts/1993.shtml">1993</a>,
      <a href="/reports/abstracts/1992.shtml">1992</a>,
      <a href="/reports/abstracts/1991.shtml">1991</a>,
      <a href="/reports/abstracts/1990.shtml">1990</a>,
      <a href="/reports/abstracts/1989.shtml">1989</a>,
      <a href="/reports/abstracts/1988.shtml">1988</a>,
      <a href="/reports/abstracts/1987.shtml">1987</a>,
      <a href="/reports/abstracts/1986.shtml">1986</a>
    </font>
  </td>
</tr>
</table>
<hr>
<!-- <table bgcolor=#lightyellow width=100%><tr><td> set the page background color -->

<font size=+2><b> BMMC Permutations on a DECmpp 12000/sx 2000 </b></font><br>
<font size=+1><b>
 <A HREF="../../authors/Bruhl,Kristin.shtml">Kristin Bruhl</A>
<br>
Dartmouth PCS-TR94-224
</b></font>
<p><QUOTE>
<b>Abstract:</b>
</P><P>
  Increasingly, modern computing problems, including many scientific
and business applications, require huge amounts of data to be
examined, modified, and stored.  Parallel computers can be used to
decrease the time needed to operate on such large data sets, by
allowing computations to be performed on many pieces of data at once.
For example, on the DECmpp machine used in our research, there are
2048 processors in the parallel processor array.  The DECmpp can read
data into each of these processors, perform a computation in parallel
on all of it, and write the data out again, theoretically decreasing
the execution time by a factor of 2048 over the time required by one
of its processors.
</P><P>
  Often, the computations that occur after the data is in the
processors involve rearranging, or permuting, the data within the
array of parallel processors.  Information moves between processors by
means of a network connecting them.  Communication through the network
can be very expensive, especially if there are many
collisions--simultaneous contentions for the same network
resource--between items of data moving from one processor to another.
When a program performs hundreds or even thousands of these
permutations during its execution, a bottleneck can occur, impeding
the overall performance of the program.
</P><P>
  Effective algorithms that decrease the time required to permute the
data within a parallel computer can yield a significant speed increase
in running programs with large data sets.  Cormen has designed
algorithms to improve performance when the data movement is defined by
certain classes of permutations.  This thesis will examine the
performance of one of these classes, the
bit-matrix-multiply/complement (BMMC) permutation, when implemented on
the DECmpp.  Although Cormen's algorithm was designed for parallel
disk systems, this thesis adapts it to permutations of data residing
in the memory of the parallel processors.
</P><P>
  The DECmpp network follows the model of an Extended Delta Network
(EDN).  One characteristic of an EDN is that it has a set of input and
output ports to the network, each of which can carry only one item of
data at a time.  If more than one item needs to travel over a given
port, a collision occurs.  The data must access the port serially,
which slows down the entire operation.  Cormen's algorithm reduces
these collisions by computing a schedule for sending the data over the
network.
</P><P>
  For small data sets, it is not worthwhile to perform the extra
operations to generate such a schedule, because the overhead
associated with computing the schedule outweighs the time gained by
preventing collisions at the network ports.  As the size of the data
set increases, eliminating collisions becomes more and more valuable.
On the DECmpp, when the data permutation involves more than 128
elements per processor, our algorithm beats the more naive and obvious
method for permuting in the parallel processor array.
</QUOTE></p>
<p><QUOTE>
<b>Note:</b>
A Senior Honors Thesis in Computer Science.  Advisor: Thomas Cormen.
</QUOTE></p>
<HR>
 <a href="http://www.cs.dartmouth.edu/reports/TR94-224.ps.Z"><img align=center alt="PS.Z" width=30 height=32 border=0 src=../../ps.gif></a>
 compressed postscript .ps.Z (192KB)
,
 <a href="http://www.cs.dartmouth.edu/reports/TR94-224.pdf"><img align=center alt="PDF" width=29 height=32 border=0 src=../../pdf.gif></a>
 PDF (360KB)
(derived from the ps.Z)
<p>
Bibliographic citation for this report:
  [<a href="TR94-224.ascii">plain text</a>]
  [<a href="TR94-224.bib">BIB</a>]
  [<a href="TR94-224.bibtex">BibTeX</a>]
  [<a href="TR94-224.refer">Refer</a>]
</p>
<p>Or copy and paste:<br>
&nbsp;&nbsp;
Kristin Bruhl, 
"BMMC Permutations on a DECmpp 12000/sx 2000."
Dartmouth Computer Science Technical Report PCS-TR94-224,
 1994.
</p>
<!-- Include file for abstracts/*.html and authors/*.html -->
<!-- See also header.htmlf -->
<!-- I use absolute pathnames here because this file is used from several directories. -->
<!-- </td></tr></table> -->
<hr>

<p><a href=/reports/updates.html>Notify me about new tech reports</a>.
<a href=/reports/feed.xml><img src=/reports/rss.gif><a></p>
<p><a href=/reports/index.shtml#search>Search the technical reports</a>.</p>
<p>To receive paper copy of a report, by mail, 
send your address and the TR number to <tt>reports</tt> AT <tt>cs.dartmouth.edu</tt>

<hr>

<p><font size=-1><b>Copyright notice</b>:	
The documents contained in this server are included by the
contributing authors as a means to ensure timely dissemination of
scholarly and technical work on a non-commercial basis.  Copyright and
all rights therein are maintained by the authors or by other copyright
holders, notwithstanding that they have offered their works here
electronically.  It is understood that all persons copying this
information will adhere to the terms and constraints invoked by each
author's copyright.  These works may not be reposted without the
explicit permission of the copyright holder.
</font></p> 
<p><font size=-1><em>Technical reports collection maintained by David Kotz</em>.

</BODY>
</HTML>
