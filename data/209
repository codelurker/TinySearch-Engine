http://www.cs.dartmouth.edu/~ac/DTS
2
<html><head><title>DTS: Dartmouth Theory Seminars - Current</title></head>
<body background="../dartback.png" link=#0000ff vlink=#0000ff>

<h2>DTS: Dartmouth Theory Seminars</h2>

Dartmouth Theory Seminars are an excellent opportunity for all students
and faculty at Dartmouth to hear about some of the latest exciting work
in Theoretical Computer Science (TCS). They are also an opporunity for
TCS researchers in other institutions to visit Dartmouth's beautiful
campus in Hanover, NH. <p>

For the current term (Fall 2006), this seminar series is organized by
<a href="http://www.cs.dartmouth.edu/~ac">Amit Chakrabarti</a>.<p>

<b>To non-Dartmouth theoreticians:</b> If you are going
to be visiting any nearby institutions (in Boston, New York, or Amherst,
say) and would like to give a talk as part of the Dartmouth Theory
Seminar series, please contact Amit. <p>

<hr><p>

<h3>Fall 2006 Schedule</h3>

<p> Schedules from past terms: 
[ <a href="fall05.html">Fall 2005</a>
| <a href="spring05.html">Spring+Summer 2005</a>
| <a href="winter05.html">Winter 2005</a>
| <a href="fall04.html">Fall 2004</a>
| <a href="spring04.html">Spring 2004</a> ]
</p>

<table cellpadding=3><tr>
  <td valign=top>Oct 4 (Wed)</td>
  <td>Josh Buresh-Oppenheim, Simon Fraser University<br>
  <a href="#buresh">Towards Models for Backtracking and Dynamic
  Programming</a><br>
  Room: Carpenter 013 (note unusual location!) <br>
  Time: 4pm - 5pm.
</tr><tr>
  <td valign=top>Oct 30 (Mon)</td>
  <td>Sidharth Jaggi, MIT<br>
  <a href="#jaggi">Fighting Byzantine Adversaries in Networks: Network
  Error-Correcting Codes</a><br>
  Room: Carson L02 <br>
  Time: 4pm - 5pm.
</tr><tr>
  <td valign=top>Nov 27 (Mon)</td>
  <td>Nick Harvey, MIT<br>
  <a href="#harvey">The Conjugate Gradient Method with Automatic
  Preconditioning</a><br>
  Room: Carson L02 <br>
  Time: 4pm - 5pm.
</tr><tr>
  <td valign=top>Dec 4 (Mon)</td>
  <td>Leslie Valiant, Harvard University<br>
  <a href="#valiant">Biology as Computation</a><br>
  Room: Carson L02 <br>
  Time: 4pm - 5pm.
</tr></table>

<p><hr><p>

<h3>Abstracts</h3>

<a name="buresh">

  <p><b>Josh Buresh-Oppenheim, Simon Fraser University<br>
  Towards Models for Backtracking and Dynamic Programming</b></p>

  <p>Since most algorithms that people use can intuitively be classified
  into large paradigms of algorithms such as greedy, dynamic
  programming, linear and semidefinite programming, local search, etc.,
  it is natural to ask what problems can be computed by these paradigms.
  This question can be seen as an alternative to asking what problems
  can be computed by all, say, poly-time algorithms in that the
  restriction on the algorithms is more conceptual rather than
  complexity-theoretic.  Of course, to ask a question about an
  algorithmic paradigm, you first have to formally define the paradigm.
  We offer one very natural model, pBT, which captures many algorithms
  generally considered to be dynamic programming or backtracking.  This
  model is an extension of the Priority Algorithm model of Borodin,
  Nielsen and Rackoff, which captures greedy algorithms.  We demonstrate
  many upper and lower bounds in this model for problems such as
  interval scheduling, knapsack and SAT.  We then define a stronger
  model, pBP, and show that it seems to capture some aspects of dynamic
  programming that pBT does not, but still does not solve even tractable
  problems that do not intuitively have dynamic programming
  algorithms.</p>

  <p>This is joint work with Mikhail Alekhnovich, Allan Borodin, Sashka
  Davis, Russell Impagliazzo, Avner Magen and Toni Pitassi.</p>

<hr width=50%><p>

<a name="jaggi">

  <p><b>Sidharth Jaggi, MIT<br>
  Fighting Byzantine Adversaries in Networks: Network Error-Correcting Codes
  </b></p>

  <p>It was shown by Ahlswede et al. that in general network coding is
  required  to  attain  the  multicast  capacity.  But since network
  coding involves mixing of information inside the network, a single
  corrupted packet generated by a malicious node can end up
  contaminating all the information reaching a destination, preventing
  decoding.   This   talk   introduces   the first distributed
  polynomial-time  rate-optimal  network  error-correcting codes that
  work in the presence of Byzantine nodes. We present algorithms that
  target adversaries with different attacking capabilities.  When the
  adversary can eavesdrop on all links and jam z links, our first
  algorithm achieves a rate of (C - 2z), where C is the network capacity.
  In contrast, when the adversary has limited snooping capabilities, we
  provide algorithms that achieve the higher rate of (C - z). Our
  algorithms attain the optimal rate given the strength of the
  adversary. They are information-theoretically secure. They can be
  designed and operated in a distributed manner, assume no knowledge of
  the topology, and can be designed and implemented in polynomial time.
  Furthermore, only the source and destination need to be modified;
  non-malicious nodes inside the network are oblivious to the presence
  of adversaries and implement a classical distributed network code.
  Finally, our algorithms work over wired and wireless networks.</p>

  <p>This is joint work done with Michael Langberg, Sachin Katti, Tracey
  Ho, Dina Katabi, and Muriel Medard.</p>

<hr width=50%><p>

<a name="harvey">

  <p><b>Nick Harvey, MIT<br>
  The Conjugate Gradient Method with Automatic Preconditioning</b></p>

  <p>Solving a linear system is one of the most fundamental
  computational problems. Unfortunately, the basic algorithm that most
  of us learn (Gaussian Elimination) is often useless in practice due to
  slow running time or stability issues. Instead, it is more common to
  use iterative solvers, the simplest ones being steepest descent and
  conjugate gradient.  The snag with iterative solvers is that their
  performance often depends on the "condition number" of the given
  system, so it is common to modify the system by applying a
  "preconditioner" matrix which reduces the condition number. This
  raises a key question: given a linear system, how can we find a good
  preconditioner?</p>

  <p>In this work, we develop a variant of conjugate gradient method
  which *automatically* constructs good preconditioners. The general
  idea is very simple. We run the conjugate gradient method until it
  "gets stuck".  The fact that it is stuck then implies a way to modify
  the preconditioner so that the conjugate gradient steps will be "less
  stuck" in the future.</p>

  <p>This talk will be self-contained -- the audience only needs to know
  basic linear algebra, and how to interpret pictures of algorithms that
  are stuck.</p>

  <p>Joint work with John Dunagan, Microsoft Research.</p>

<hr width=50%><p>

<a name="valiant">

  <p><b>Leslie Valiant, Harvard University<br>
  Biology as Computation</b></p>

  <p> We argue that computational models have an essential role in
  uncovering the principles behind a variety of biological phenomena. In
  particular we consider recent results relating to the following three
  questions: How can brains, given their known resource constraints such
  as the sparsity of connections and slow elements, do any significant
  information processing at all? How can evolution, in only a few
  billion years, evolve such complex mechanisms as it apparently has?
  How can cognitive systems manipulate large amounts of such uncertain
  knowledge and get usefully reliable results? We show that each of
  these problems can be formulated as a quantitative question for a
  computational model, and argue that solutions to these formulations
  provide some understanding of these biological phenomena. </p>

  <p>This talk will be accessible to graduate <i>and</i> undergraduate 
  students. </p>

<hr><p>

<p align=right> <i> Back to <a href="../index.html">Amit Chakrabarti's
home page</a>. </i> </p>

</body></html>
